apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: qwen
  namespace: demo-llm
spec:
  replicas: 4
  model:
    uri: hf://Qwen/Qwen3-4B
    name: Qwen/Qwen3-4B
  router:
    scheduler:
      template:
        containers:
          - name: main
            args:
              - -v=4 # Debug level
              - --poolName
              - "{{ ChildName .ObjectMeta.Name `-inference-pool` }}"
              - --poolNamespace
              - "{{ .ObjectMeta.Namespace }}"
              - --zap-encoder
              - json
              - --grpcPort
              - "9002"
              - --grpcHealthPort
              - "9003"
              - --secureServing
              - --modelServerMetricsScheme
              - https
              - --modelServerMetricsHttpsInsecureSkipVerify
              - --certPath
              - /etc/ssl/certs
              - --configText
              - |2

                apiVersion: inference.networking.x-k8s.io/v1alpha1
                kind: EndpointPickerConfig
                plugins:
                - type: single-profile-handler
                - type: max-score-picker
                - type: queue-scorer
                - type: kv-cache-scorer
                - type: prefix-cache-scorer
                  parameters:
                    hashBlockSize: 64             # block size 16 * 4 chars / token
                    maxPrefixBlocksToMatch: 256   # this matches up to ~4k tokens
                    lruCapacityPerServer: 39744   # total blocks (635904) / block size (16)
                schedulingProfiles:
                - name: default
                  plugins:
                    - pluginRef: prefix-cache-scorer
                      weight: 2.0
                    - pluginRef: queue-scorer
                      weight: 1.0
                    - pluginRef: kv-cache-scorer
                      weight: 1.0
                    - pluginRef: max-score-picker
    route: { }
    gateway: { }
  template:
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    containers:
      - name: main
        env:
          - name: VLLM_ADDITIONAL_ARGS
            value: "--disable-uvicorn-access-log"
        resources:
          limits:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '1'
            memory: 8Gi
            nvidia.com/gpu: "1"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 30
          failureThreshold: 5
